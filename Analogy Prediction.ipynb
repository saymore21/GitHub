{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sayali\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.deprecated.keyedvectors import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "input_file_glove = 'glove.840B.300d.txt'\n",
    "output_file_word2vec ='glove.840B.300d.txt.word2vec'\n",
    "glove2word2vec(input_file_glove, output_file_word2vec)\n",
    "\n",
    "Word2Vec_Model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "Glove_Model = KeyedVectors.load_word2vec_format(output_file_word2vec, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "file = open(\"word-test.v1.txt\")\n",
    "file_text = file.read()\n",
    "patterns = []\n",
    "patterns = [r\"(?<=:capital-world)((\\w|\\s)*)(?=:)\",  r\"(?<=: currency)((\\w|\\s)*)(?=:)\", r\"(?<=: city-in-state)((\\w|\\s)*)(?=:)\",\n",
    "                  r\"(?<=: family)((\\w|\\s)*)(?=:)\", r\"(?<=: gram1-adjective-to-adverb)((\\w|\\s)*)(?=:)\", r\"(?<=: gram2-opposite)((\\w|\\s)*)(?=:)\",\n",
    "                  r\"(?<=: gram3-comparative)((\\w|\\s)*)(?=:)\", r\"(?<=: gram6-nationality-adjective)((\\w|\\s)*)(?=:)\"] \n",
    "\n",
    "data = ''\n",
    "for regex in patterns:\n",
    "    rmatch =(re.finditer(regex, file_text))\n",
    "    for i in rmatch:\n",
    "        data += i.group().strip()\n",
    "        \n",
    "        \n",
    "data = re.sub('[ \\t]+' , ' ', data)\n",
    "\n",
    "Count_Word2Vec = 0\n",
    "Count_Glove = 0\n",
    "Datacount = 0\n",
    "\n",
    "for line in data.split('\\n'):\n",
    "    if line != \" \":\n",
    "#         print(line)\n",
    "        words = line.split(\" \")\n",
    "        Datacount += 1\n",
    "        Word2Vec_Output = Word2Vec_Model.most_similar(positive=[words[2], words[1]], negative=[words[0]], topn=1)\n",
    "#         #print(resultWord2Vec)\n",
    "        interpreted_word_w2v =  Word2Vec_Output[0][0]\n",
    "        if interpreted_word_w2v.lower() == words[3].lower():\n",
    "            Count_Word2Vec += 1\n",
    "        Glove_output = Glove_Model.most_similar(positive=[words[2], words[1]], negative=[words[0]], topn=1)\n",
    "        #print(resultGlove)\n",
    "        interpreted_word_glove = Glove_output[0][0]\n",
    "        if interpreted_word_glove.lower() == words[3].lower():\n",
    "            Count_Glove += 1\n",
    "\n",
    "\n",
    "print(\"Word2Vec Model Output: \")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "Accuracy_Word2Vec = (Count_Word2Vec*100)/Datacount\n",
    "print(\"word2Vec Count: \",Count_Word2Vec)\n",
    "print(\"word2Vec Accuracy: \",Accuracy_Word2Vec)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Glove Model Output: \")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "Accuracy_Glove = (Count_Glove*100)/Datacount\n",
    "print(\"Glove Count: \",Count_Glove)\n",
    "print(\"Glove Accuracy: \",Accuracy_Glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding selection:\n",
    "I have chosen the two embedding techniques of Glove and Word2Vec as they have the advantage of Pretrained word embeddings which can be used directly for training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for increase:  [('decrease', 0.8370318412780762), ('increases', 0.7709376811981201), ('increased', 0.7578041553497314), ('reduction', 0.6908220648765564), ('increasing', 0.6871616244316101), ('decreases', 0.681617259979248), ('rise', 0.6352647542953491), ('decreasing', 0.621862530708313), ('decline', 0.6128641366958618), ('increasein', 0.5982418060302734)]\n",
      "Cosine similarity for open:  [('opened', 0.596660315990448), ('closed', 0.5819659233093262), ('opens', 0.5362014770507812), ('opening', 0.5158241391181946), ('reopen', 0.5051860809326172), ('closes', 0.46577221155166626), ('close', 0.46365123987197876), ('9am_5pm_Mon', 0.4396822452545166), ('reopened', 0.433058500289917), ('ajar', 0.4187529683113098)]\n",
      "Cosine similarity for high:  [('low', 0.7421891689300537), ('High', 0.5790751576423645), ('higher', 0.5694516897201538), ('highest', 0.5553146600723267), ('ahigh', 0.5219955444335938), ('lower', 0.5140001773834229), ('Low', 0.5026226043701172), ('HIGH', 0.479312926530838), ('lowest', 0.4690745174884796), ('elevated', 0.4603397250175476)]\n",
      "Cosine similarity for up:  [('down', 0.6396992802619934), ('out', 0.5464873313903809), ('off', 0.5370627045631409), ('ups', 0.482612282037735), ('upthe', 0.47866737842559814), ('in.', 0.4756893813610077), ('up.The', 0.4518883228302002), ('around', 0.4468981623649597), ('aside', 0.440209299325943), ('slack_Drevna', 0.43879854679107666)]\n",
      "Cosine similarity for yes:  [('mso_style_qformat', 0.7334200143814087), ('Yes', 0.7293006777763367), ('mso_style_noshow', 0.7242437601089478), ('resounding_YES', 0.6525195240974426), ('emerge_victorious_Inouye', 0.6478173732757568), ('yeah', 0.6455683708190918), ('resounding_NO', 0.6274983882904053), ('Well_duh', 0.622299075126648), ('Uh', 0.621929943561554), ('Oh_yes', 0.6162091493606567)]\n",
      "Cosine similarity for big:  [('huge', 0.7809855937957764), ('bigger', 0.6842385530471802), ('biggest', 0.6336528658866882), ('major', 0.6057686805725098), ('gigantic', 0.5822017192840576), ('HUGE', 0.5604170560836792), ('large', 0.5561479330062866), ('humongous', 0.5558628439903259), ('Huge', 0.5524426102638245), ('great', 0.5486619472503662)]\n",
      "Cosine similarity for minimum:  [('maximum', 0.6927940845489502), ('Minimum', 0.652877926826477), ('mininum', 0.6474853754043579), ('minumum', 0.6377664804458618), ('mimimum', 0.6248406767845154), ('minium', 0.6225990056991577), ('miniumum', 0.5759451389312744), ('maximum_allowable', 0.5688034296035767), ('minimums', 0.5594750642776489), ('lawsuits_Skokan', 0.5539460182189941)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity for increase: \",Word2Vec_Model.similar_by_word(word = 'increase', topn=10))\n",
    "print(\"---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"Cosine similarity for open: \",Word2Vec_Model.similar_by_word(word = 'open', topn=10))\n",
    "print(\"---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"Cosine similarity for high: \",Word2Vec_Model.similar_by_word(word = 'high', topn=10))\n",
    "print(\"---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"Cosine similarity for up: \",Word2Vec_Model.similar_by_word(word = 'up', topn=10))\n",
    "print(\"---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"Cosine similarity for yes: \",Word2Vec_Model.similar_by_word(word = 'yes', topn=10))\n",
    "print(\"---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"Cosine similarity for big: \",Word2Vec_Model.similar_by_word(word = 'big', topn=10))\n",
    "print(\"---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"Cosine similarity for minimum: \",Word2Vec_Model.similar_by_word(word = 'minimum', topn=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question :\n",
    "Discuss why embeddings might have this tendency\n",
    "Word embeddings are created depending upon the vocabulary of the entire document and they depend on the context of the document. Generally the antonyms are used in the same context or they are show syntactic similarity. For example, I hate you and I love you  or Increase in prices or Decrease in  prices if taken as documents as can be seen they are surrounded by same words so they will end up having same vectors as the vectors formed depend on the vocabulary of the document. Thus this gneral tendency of antonyms of having similar surrounding words or appearing in similar context makes them have same vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analogy Dataset \n",
    "# Relation #1 - Animal - Language\n",
    "# Relation #2 - Singular - Plural\n",
    "\n",
    "Analogy_Data = ['cat meows dog barks','cow moo lion roar', 'man speaks bird chirps', \n",
    "               'ship fleet cow herd','lion pride ant colony','flower bouquet oxen yoke']\n",
    "\n",
    "Count_Word2Vec = 0\n",
    "Count_Glove = 0\n",
    "Datacount = 0\n",
    "\n",
    "\n",
    "for line in Analogy_Data:\n",
    "    words = line.split(\" \")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    Datacount += 1\n",
    "    Word2Vec_Output = Word2Vec_Model.most_similar(positive=[words[2], words[1]], negative=[words[0]], topn=1)\n",
    "    print(Word2Vec_Output)\n",
    "    interpreted_word_w2v =  Word2Vec_Output[0][0]\n",
    "    if interpreted_word_w2v.lower() == words[3].lower():\n",
    "        Count_Word2Vec += 1\n",
    "    Glove_output = Glove_Model.most_similar(positive=[words[2], words[1]], negative=[words[0]], topn=1)\n",
    "    print(Glove_output)\n",
    "    interpreted_word_glove = Glove_output[0][0]\n",
    "    if interpreted_word_glove.lower() == words[3].lower():\n",
    "        Count_Glove += 1\n",
    "\n",
    "print(\"Word2Vec Model Output: \")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "Accuracy_Word2Vec = (Count_Word2Vec*100)/Datacount\n",
    "print(\"word2Vec Count: \",Count_Word2Vec)\n",
    "print(\"word2Vec Accuracy: \",Accuracy_Word2Vec)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Glove Model Output: \")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "Accuracy_Glove = (Count_Glove*100)/Datacount\n",
    "print(\"Glove Count: \",Count_Glove)\n",
    "print(\"Glove Accuracy: \",Accuracy_Glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
